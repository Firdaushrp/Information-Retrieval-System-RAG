{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8066ad30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Memuat dataset JSON: News_Category_Dataset_v3.json...\n",
      "   -> JSON OK: 209527 baris.\n",
      "2. Memuat dataset CSV: final_merge_dataset.csv...\n",
      "   -> Kolom Asli: ['Judul', 'Waktu', 'Link', 'Content', 'tag1', 'tag2', 'tag3', 'tag4', 'tag5', 'source']\n",
      "   -> Menggabungkan Tags...\n",
      "   -> Membersihkan artefak teks...\n",
      "   -> CSV OK: 80472 baris.\n",
      "3. Menggabungkan & Memproses Data Akhir...\n",
      "4. Menyimpan 254032 data ke 'processed_news_data.csv'...\n",
      "\n",
      "--- CONTOH DATA HASIL CLEANING ---\n",
      "Kategori: israel, gaza, global-sumud-flotilla, kapal, flotilla\n",
      "Judul: Protes Meletus di Seluruh Eropa Usai Serangan Israel terhadap Kapal Flotilla ke Gaza\n",
      "Isi: Baca berita dengan sedikit iklan, klik di sini Aktivis pro-Palestina menggelar protes pada Rabu malam di seluruh Eropa seperti dilaporkan Anadolu menyusul serangan pasukan Israel terhadap armada Global Sumud Flotilla, armada kapal yang membawa bantuan kemanusiaan ke Gaza, Palestina. Baca berita dengan sedikit iklan, klik di sini Di Roma, ratusan demonstran, termasuk mahasiswa dan anggota serikat akar rumput, berkumpul di Piazza dei Cinquecento di depan Stasiun Termini. Scroll ke bawah untuk melanjutkan membaca Baca berita dengan sedikit iklan, klik di sini Para demonstran memblokir lalu lintas di alun-alun dan jalan-jalan sekitarnya, meneriakkan slogan-slogan seperti \"Mari kita blokir segalanya demi Flotilla dan demi Palestina.\" Polisi Italia menutup beberapa stasiun metro dan membatasi akses penumpang ke Stasiun Termini. Para demonstran berencana berbaris menuju Piazza Barberini dengan sekitar seribu peserta. Serikat pekerja Italia, Unione Sindacale di Base (USB) dan Confederazione Generale Italiana del Lavoro (CGIL), menyerukan pemogokan umum nasional pada Jumat 3 Oktober 2025. Di Barcelona, ratusan orang berkumpul di luar konsulat Israel untuk mengecam intersepsi dan menyatakan ...\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Konfigurasi file\n",
    "FILE_LAMA_JSON = 'News_Category_Dataset_v3.json'       \n",
    "FILE_BARU_CSV = 'final_merge_dataset.csv'\n",
    "OUTPUT_FILE = 'processed_news_data.csv'\n",
    "\n",
    "def clean_text_artifacts(text):\n",
    "    \"\"\"\n",
    "    Membersihkan karakter encoding yang rusak (Mojibake) dan whitespace berlebih.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    #Perbaikan Karakter Aneh\n",
    "    replacements = {\n",
    "        'â€“': '-', 'â€”': '-', \n",
    "        'â€œ': '\"', 'â€': '\"',  \n",
    "        'â€™': \"'\", 'â€˜': \"'\", \n",
    "        'Â': '', 'â': '',       \n",
    "        '\\xa0': ' '             \n",
    "    }\n",
    "    for bad, good in replacements.items():\n",
    "        text = text.replace(bad, good)\n",
    "        \n",
    "    #Hapus Intro Berita\n",
    "    text = re.sub(r'^[A-Z\\s,]+(\\.|kompas\\.com|detik\\.com)\\s*(--|-)\\s*', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    #Hapus pola\n",
    "    text = re.sub(r'\\[Gambas:.*?\\]', '', text)\n",
    "    \n",
    "    #Rapikan Spasi\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def combine_tags(row):\n",
    "    \"\"\"Menggabungkan tag1 s/d tag5 menjadi satu string kategori\"\"\"\n",
    "    tags = []\n",
    "    for i in range(1, 6):\n",
    "        col = f'tag{i}'\n",
    "        if col in row and pd.notna(row[col]) and str(row[col]).strip() != 'nan':\n",
    "            tags.append(str(row[col]).strip())\n",
    "\n",
    "    if not tags:\n",
    "        if 'source' in row and pd.notna(row['source']):\n",
    "            return str(row['source'])\n",
    "        return \"General\"\n",
    "        \n",
    "    return \", \".join(tags)\n",
    "\n",
    "def load_and_process():\n",
    "    dfs = []\n",
    "\n",
    "    #Dataset Lama (JSON)\n",
    "    print(f\"1. Memuat dataset JSON: {FILE_LAMA_JSON}...\")\n",
    "    if os.path.exists(FILE_LAMA_JSON):\n",
    "        try:\n",
    "            df_old = pd.read_json(FILE_LAMA_JSON, lines=True)\n",
    "            df_old = df_old[['headline', 'short_description', 'link', 'category']]\n",
    "            dfs.append(df_old)\n",
    "            print(f\"   -> JSON OK: {len(df_old)} baris.\")\n",
    "        except Exception as e:\n",
    "            print(f\"   -> Error JSON: {e}\")\n",
    "    else:\n",
    "        print(f\"   -> File JSON tidak ditemukan (Dilewati).\")\n",
    "\n",
    "    #Dataset Baru (CSV)\n",
    "    print(f\"2. Memuat dataset CSV: {FILE_BARU_CSV}...\")\n",
    "    if os.path.exists(FILE_BARU_CSV):\n",
    "        try:\n",
    "            try:\n",
    "                df_new = pd.read_csv(FILE_BARU_CSV, encoding='utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                df_new = pd.read_csv(FILE_BARU_CSV, encoding='latin-1')\n",
    "\n",
    "            print(f\"   -> Kolom Asli: {list(df_new.columns)}\")\n",
    "\n",
    "            #Gabungkan Tags menjadi Category\n",
    "            print(\"   -> Menggabungkan Tags...\")\n",
    "            df_new['category'] = df_new.apply(combine_tags, axis=1)\n",
    "\n",
    "            #Mapping Nama Kolom agar sama dengan JSON\n",
    "            rename_map = {\n",
    "                'Judul': 'headline',\n",
    "                'Content': 'short_description',\n",
    "                'Link': 'link',\n",
    "            }\n",
    "            df_new = df_new.rename(columns=rename_map)\n",
    "\n",
    "            #Bersihkan Teks (Cleaning)\n",
    "            print(\"   -> Membersihkan artefak teks...\")\n",
    "            df_new['headline'] = df_new['headline'].apply(clean_text_artifacts)\n",
    "            df_new['short_description'] = df_new['short_description'].apply(clean_text_artifacts)\n",
    "\n",
    "            #Seleksi Kolom\n",
    "            df_new = df_new[['headline', 'short_description', 'link', 'category']]\n",
    "            \n",
    "            dfs.append(df_new)\n",
    "            print(f\"   -> CSV OK: {len(df_new)} baris.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   -> Error CSV: {e}\")\n",
    "    else:\n",
    "        print(f\"   -> File CSV tidak ditemukan (Dilewati).\")\n",
    "\n",
    "    #Penggabungan \n",
    "    if not dfs:\n",
    "        print(\"❌ Tidak ada data untuk diproses.\")\n",
    "        return\n",
    "\n",
    "    print(\"3. Menggabungkan & Memproses Data Akhir...\")\n",
    "    df_final = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    #Hapus Duplikat & Kosong\n",
    "    df_final.drop_duplicates(subset=['headline'], inplace=True)\n",
    "    df_final = df_final.dropna(subset=['headline', 'short_description'])\n",
    "    df_final = df_final[df_final['short_description'].str.len() > 10]\n",
    "\n",
    "    #Buat Combined Text untuk RAG\n",
    "    def format_rag_text(row):\n",
    "        \n",
    "        content = str(row['short_description'])\n",
    "        if len(content) > 1200:\n",
    "            content = content[:1200] + \"...\"\n",
    "        \n",
    "        return f\"Kategori: {row['category']}\\nJudul: {row['headline']}\\nIsi: {content}\"\n",
    "\n",
    "    df_final['combined_text'] = df_final.apply(format_rag_text, axis=1)\n",
    "\n",
    "    #Buat Doc ID Baru\n",
    "    df_final['doc_id'] = [f\"doc_{i}\" for i in range(len(df_final))]\n",
    "\n",
    "    #Simpan\n",
    "    print(f\"4. Menyimpan {len(df_final)} data ke '{OUTPUT_FILE}'...\")\n",
    "    df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    #Preview Data\n",
    "    print(\"\\n--- CONTOH DATA HASIL CLEANING ---\")\n",
    "    print(df_final.iloc[-1]['combined_text']) \n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a020e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Memuat data CSV...\n",
      "   Total dokumen: 254032\n",
      "2. Menyiapkan dokumen...\n",
      "3. Memuat Model Embedding (sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\RAG_NEW\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Membuat Index FAISS Baru (Tunggu sebentar)...\n",
      "5. Menyimpan Index ke 'faiss_index_news'...\n",
      "\n",
      "✅ SUKSES! Index baru berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    from langchain_core.documents import Document\n",
    "except ImportError as e:\n",
    "    print(f\"Error Import: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "#Konfigurasi\n",
    "DATA_PATH = 'processed_news_data.csv' \n",
    "INDEX_PATH = 'faiss_index_news'       \n",
    "MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "def main():\n",
    "    #Cek File CSV\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        print(f\"❌ ERROR: File '{DATA_PATH}' tidak ditemukan.\")\n",
    "        print(\"   Pastikan Anda sudah menjalankan 'preprocess_gabungan.py'.\")\n",
    "        return\n",
    "\n",
    "    print(\"1. Memuat data CSV...\")\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_PATH)\n",
    "        df = df.dropna(subset=['combined_text'])\n",
    "\n",
    "        print(f\"   Total dokumen: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Gagal membaca CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"2. Menyiapkan dokumen...\")\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        doc = Document(\n",
    "            page_content=row['combined_text'],\n",
    "            metadata={\"doc_id\": row['doc_id'], \"category\": row.get('category', 'unknown')}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    print(f\"3. Memuat Model Embedding ({MODEL_NAME})...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "    print(\"4. Membuat Index FAISS Baru (Tunggu sebentar)...\")\n",
    "    if os.path.exists(INDEX_PATH):\n",
    "        try:\n",
    "            shutil.rmtree(INDEX_PATH)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    \n",
    "    print(f\"5. Menyimpan Index ke '{INDEX_PATH}'...\")\n",
    "    vectorstore.save_local(INDEX_PATH)\n",
    "    print(\"\\n✅ SUKSES! Index baru berhasil dibuat.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db4b770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.1.0\n",
      "  Using cached langchain-0.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain-community==0.0.10\n",
      "  Using cached langchain_community-0.0.10-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting langchain-core==0.1.10\n",
      "  Using cached langchain_core-0.1.10-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.13.0-cp311-cp311-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.9.1-cp311-cp311-win_amd64.whl.metadata (30 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: ipykernel in c:\\rag_new\\venv\\lib\\site-packages (7.1.0)\n",
      "Collecting pydantic<2.0.0\n",
      "  Using cached pydantic-1.10.24-cp311-cp311-win_amd64.whl.metadata (156 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain==0.1.0)\n",
      "  Using cached pyyaml-6.0.3-cp311-cp311-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.1.0)\n",
      "  Using cached sqlalchemy-2.0.44-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.1.0)\n",
      "  Using cached aiohttp-3.13.2-cp311-cp311-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.0)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.1.0)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.77 (from langchain==0.1.0)\n",
      "  Using cached langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting numpy<2,>=1 (from langchain==0.1.0)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting requests<3,>=2 (from langchain==0.1.0)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.1.0)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyio<5,>=3 (from langchain-core==0.1.10)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core==0.1.10)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.16.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.1.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-12.0.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\rag_new\\venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.6-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\rag_new\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\rag_new\\venv\\lib\\site-packages (from ipykernel) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\rag_new\\venv\\lib\\site-packages (from ipykernel) (1.8.17)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\rag_new\\venv\\lib\\site-packages (from ipykernel) (9.7.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\rag_new\\venv\\lib\\site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\rag_new\\venv\\lib\\site-packages (from ipykernel) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\rag_new\\venv\\lib\\site-packages (from ipykernel) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\rag_new\\venv\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\rag_new\\venv\\lib\\site-packages (from ipykernel) (7.1.3)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\rag_new\\venv\\lib\\site-packages (from ipykernel) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\rag_new\\venv\\lib\\site-packages (from ipykernel) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\rag_new\\venv\\lib\\site-packages (from ipykernel) (5.14.3)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
      "  Using cached frozenlist-1.8.0-cp311-cp311-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
      "  Using cached multidict-6.7.0-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
      "  Using cached propcache-0.4.1-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0)\n",
      "  Using cached yarl-1.22.0-cp311-cp311-win_amd64.whl.metadata (77 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3->langchain-core==0.1.10)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3->langchain-core==0.1.10)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.0)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.0)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting shellingham (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: colorama>=0.4.4 in c:\\rag_new\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: decorator>=4.3.2 in c:\\rag_new\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in c:\\rag_new\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in c:\\rag_new\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\rag_new\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in c:\\rag_new\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in c:\\rag_new\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.1.0)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\rag_new\\venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\rag_new\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2->langchain==0.1.0)\n",
      "  Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl.metadata (38 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain==0.1.0)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain==0.1.0)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain==0.1.0)\n",
      "  Using cached greenlet-3.2.4-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached regex-2025.11.3-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached markupsafe-3.0.3-cp311-cp311-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\rag_new\\venv\\lib\\site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel) (0.8.5)\n",
      "Requirement already satisfied: wcwidth in c:\\rag_new\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.14)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\rag_new\\venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\rag_new\\venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\rag_new\\venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.0)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Using cached langchain-0.1.0-py3-none-any.whl (797 kB)\n",
      "Using cached langchain_community-0.0.10-py3-none-any.whl (1.5 MB)\n",
      "Using cached langchain_core-0.1.10-py3-none-any.whl (216 kB)\n",
      "Using cached faiss_cpu-1.13.0-cp311-cp311-win_amd64.whl (18.7 MB)\n",
      "Using cached sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Using cached torch-2.9.1-cp311-cp311-win_amd64.whl (111.0 MB)\n",
      "Using cached pandas-2.3.3-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "Using cached pydantic-1.10.24-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Using cached aiohttp-3.13.2-cp311-cp311-win_amd64.whl (456 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "Downloading networkx-3.6-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.1 MB 640.0 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.1/2.1 MB 812.7 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/2.1 MB 1.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.1 MB 888.4 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.5/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.1 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/2.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.1 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/2.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.1 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.4/2.1 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.4/2.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.5/2.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.1 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.7/2.1 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.7/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.7/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.7/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.7/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.7/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.9/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.9/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.9/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.9/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.9/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.0/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.0/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 1.2 MB/s eta 0:00:00\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached pyyaml-6.0.3-cp311-cp311-win_amd64.whl (158 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached sqlalchemy-2.0.44-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached pillow-12.0.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "Using cached scikit_learn-1.7.2-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "Using cached scipy-1.16.3-cp311-cp311-win_amd64.whl (38.7 MB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl (106 kB)\n",
      "Using cached frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Using cached greenlet-3.2.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached markupsafe-3.0.3-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.7.0-cp311-cp311-win_amd64.whl (46 kB)\n",
      "Using cached propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Using cached regex-2025.11.3-cp311-cp311-win_amd64.whl (277 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: pytz, mpmath, urllib3, tzdata, tqdm, threadpoolctl, tenacity, sympy, sniffio, safetensors, regex, PyYAML, pydantic, propcache, Pillow, packaging, numpy, networkx, mypy-extensions, multidict, MarkupSafe, jsonpointer, joblib, idna, greenlet, fsspec, frozenlist, filelock, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, scipy, requests, pandas, marshmallow, jsonpatch, jinja2, faiss-cpu, anyio, aiosignal, torch, scikit-learn, langsmith, huggingface-hub, dataclasses-json, aiohttp, tokenizers, langchain-core, transformers, langchain-community, sentence-transformers, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "Successfully installed MarkupSafe-3.0.3 Pillow-12.0.0 PyYAML-6.0.3 SQLAlchemy-2.0.44 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.11.0 attrs-25.4.0 certifi-2025.11.12 charset_normalizer-3.4.4 dataclasses-json-0.6.7 faiss-cpu-1.13.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.10.0 greenlet-3.2.4 huggingface-hub-0.36.0 idna-3.11 jinja2-3.1.6 joblib-1.5.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.1.0 langchain-community-0.0.10 langchain-core-0.1.10 langsmith-0.0.92 marshmallow-3.26.1 mpmath-1.3.0 multidict-6.7.0 mypy-extensions-1.1.0 networkx-3.6 numpy-1.26.4 packaging-23.2 pandas-2.3.3 propcache-0.4.1 pydantic-1.10.24 pytz-2025.2 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 scikit-learn-1.7.2 scipy-1.16.3 sentence-transformers-5.1.2 sniffio-1.3.1 sympy-1.14.0 tenacity-8.5.0 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.9.1 tqdm-4.67.1 transformers-4.57.1 typing-inspect-0.9.0 tzdata-2025.2 urllib3-2.5.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install versi stabil yang kita gunakan (LangChain 0.1.0 & Pydantic 1.x)\n",
    "%pip install langchain==0.1.0 langchain-community==0.0.10 langchain-core==0.1.10 faiss-cpu sentence-transformers torch pandas ipykernel \"pydantic<2.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b44e075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Ver  : 0.1.0\n",
      "Community Ver  : 0.0.10\n",
      "Pydantic Ver   : 1.10.24\n",
      "Pandas Ver     : 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import langchain_community\n",
    "import pydantic\n",
    "import pandas\n",
    "\n",
    "print(f\"LangChain Ver  : {langchain.__version__}\")\n",
    "print(f\"Community Ver  : {langchain_community.__version__}\")\n",
    "print(f\"Pydantic Ver   : {pydantic.VERSION}\")\n",
    "print(f\"Pandas Ver     : {pandas.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
